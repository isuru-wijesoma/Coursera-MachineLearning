{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Linear regression with multiple variables\n",
    "\n",
    "The code below represents my implementation of linear regression with multiple variables with Python. **Vectorized** solutions only are provided below.\n",
    "\n",
    "Jupyter notebook template modified from: https://github.com/dibgerge/ml-coursera-python-assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature normalization\n",
    "\n",
    "Based on ex1_multi.m, we could see that that house sizes are nearly 1000 times the number of bedrooms. Feature scaling can make gradient descent converge more quickly.\n",
    "\n",
    "\n",
    "**Implementation note:** When normalizing the features, it is important\n",
    "to store the values used for normalization - the mean value and the stan\u0002dard deviation used for the computations. After learning the parameters\n",
    "from the model, we often want to predict the prices of houses we have not\n",
    "seen before. Given a new x value (living room area and number of bed\u0002rooms), we must first normalize x using the mean and standard deviation\n",
    "that we had previously computed from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('ex1data2.txt', header=None, names=['Area', 'Bedrooms', 'Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(df):\n",
    "    \"\"\"\n",
    "    Normalizes the features in X. returns a normalized version of X where\n",
    "    the mean value of each feature is 0 and the standard deviation\n",
    "    is 1. This is often a good preprocessing step to do when working with\n",
    "    learning algorithms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        The dataset of shape (m x n).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : array_like\n",
    "        The normalized dataset of shape (m x n).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    First, for each feature dimension, compute the mean of the feature\n",
    "    and subtract it from the dataset, storing the mean value in mu. \n",
    "    Next, compute the  standard deviation of each feature and divide\n",
    "    each feature by it's standard deviation, storing the standard deviation \n",
    "    in sigma. \n",
    "    \n",
    "    Note that X is a matrix where each column is a feature and each row is\n",
    "    an example. You need to perform the normalization separately for each feature. \n",
    "\n",
    "    \"\"\"\n",
    "    m = len(df)\n",
    "    X = np.column_stack((df['Area'], df['Bedrooms']))\n",
    "    mu = X.mean(axis=0)\n",
    "    sigma = X.std(axis=0, ddof=1)\n",
    "    \n",
    "    # Feature normalization with standard deviation\n",
    "    X = (X - mu) / sigma\n",
    "    X = np.column_stack((np.ones(m), X)) # Add feature of ones\n",
    "    \n",
    "    return X, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, mu, sigma = featureNormalize(df)\n",
    "y = df['Price'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Hypothesis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X, theta):\n",
    "    \"\"\"\n",
    "    Computes the hypothesis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features. \n",
    "    \n",
    "    theta : np.array\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (n, 1).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    hypothesis : np.array\n",
    "        Prediction using theta and X.\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    return X.dot(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute cost for linear regression. \n",
    "    Computes the cost of using theta as the parameter for \n",
    "    linear regression to fit the data points in X and y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features.\n",
    "    \n",
    "    y : np.array\n",
    "        The values of the function at each data point. \n",
    "        This is a vector of shape (m, 1).\n",
    "    \n",
    "    theta : np.array\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (n, 1).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The value of the regression cost function.\n",
    "    \n",
    "    \"\"\" \n",
    "    m = len(y)\n",
    "    h = hypothesis(X, theta)\n",
    "    J = ((h - y).T).dot(h - y) / (2 * m)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentMulti(X, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `theta`. Updates theta by taking \n",
    "    `iterations` gradient steps with learning rate `alpha`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features.\n",
    "    \n",
    "    y : np.array\n",
    "        The values of the function at each data point. \n",
    "        This is a vector of shape (m, 1).\n",
    "    \n",
    "    theta : np.array\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (n, 1).\n",
    "    \n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "    \n",
    "    iterations : int\n",
    "        The number of iterations for gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : np.array\n",
    "        The learned linear regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    J_history : np.array\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(y)\n",
    "    J_history = np.zeros((iterations, 1))\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        h = hypothesis(X, theta)\n",
    "        theta -= ((h - y).dot(X)) * (alpha / m)\n",
    "        J_history[i] = computeCost(X, y, theta)\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta found by gradient descent: 340412.66, 110631.05, -6649.47\n"
     ]
    }
   ],
   "source": [
    "# -------- Run gradient descent ------------ #\n",
    "#  Choose some alpha value\n",
    "alpha = 0.15;\n",
    "num_iters = 400;\n",
    "\n",
    "# Initialize theta and run gradient descent\n",
    "theta = np.zeros(3)\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
    "\n",
    "print(f'Theta found by gradient descent: {theta[0]:.2f}, {theta[1]:.2f}, {theta[2]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): $293081.46\n"
     ]
    }
   ],
   "source": [
    "# -------- Predict price ------------ #\n",
    "test_data  = (np.array((1650, 3)) - mu) / sigma\n",
    "X_test = np.hstack((np.ones(1), test_data))\n",
    "price = X_test.dot(theta)\n",
    "\n",
    "print (f'Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ${price:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Convergence graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def plotConvergenceGraph(X, y, iterations):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `theta`. Updates theta by taking \n",
    "    `iterations` gradient steps with learning rate `alpha`.\n",
    "    Yields a graph to demonstrate change in J.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features.\n",
    "    \n",
    "    y : np.array\n",
    "        The values of the function at each data point. \n",
    "        This is a vector of shape (m, 1).\n",
    "    \n",
    "    iterations : int\n",
    "        The number of iterations for gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fig: matplotlib.pyplot graph object\n",
    "        A graph of J with different learning rates\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = [0.1, 0.03, 0.01, 0.003]\n",
    "    theta = np.zeros(3)\n",
    "    \n",
    "    J = defaultdict(list)\n",
    "    \n",
    "    for i in range(len(alpha)):\n",
    "        theta, J_history = gradientDescentMulti(X, y, theta, alpha[i], iterations)\n",
    "        J[alpha[i]].append(J_history)\n",
    "    \n",
    "    for key, value in J.items():\n",
    "        plt.plot([i for i in range(iterations)], value[0], label=key)\n",
    "    \n",
    "    plt.ylabel(r'J($\\theta$)')\n",
    "    plt.xlabel('Iterations')    \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAERCAYAAAB4jRxOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjDUlEQVR4nO3de3hddZ3v8fd379zvNJfekl7SFEpb2iIptCCIiAUqU5wZRS4iDB4YfBB09OAw+ng44DDDzKhHHYraw0U5B+vMEbGM0ELRVlDpnV7TlvQCNL0lTUjapk1z+50/9k5JS9qmSfZaO2t9Xs+zn7332uvyXQqfLH5rre8y5xwiIhJ8Eb8LEBERbyjwRURCQoEvIhISCnwRkZBQ4IuIhIQCX0QkJJI+8M3saTOrNbONvZj3CjNbY2btZvaZk3673cyq46/bE1exiEhySvrAB34GXNvLed8D7gB+0X2imQ0BHgIuAS4GHjKzcwauRBGR5Jf0ge+cex1o6D7NzMaZ2SIzW21mb5jZhPi87zjn1gOdJ63mGmCxc67BOfc+sJje/xEREQmEFL8L6KN5wD3OuWozuwR4ArjqNPOPBHZ1+14TnyYiEhqDLvDNLAe4FPh/ZtY1Of1Mi/UwTT0lRCRUBl3gExuGanTOTTuLZWqAK7t9LwWWDlxJIiLJL+nH8E/mnDsI7DSzzwJYzNQzLPYKMMvMzomfrJ0VnyYiEhpJH/hmNh94EzjPzGrM7IvArcAXzWwdsAm4IT7vdDOrAT4L/NTMNgE45xqA7wAr469H4tNERELD1B5ZRCQckv4IX0REBkZSn7QtKipyY8aM8bsMEZFBY/Xq1Qecc8U9/ZbUgT9mzBhWrVrldxkiIoOGmb17qt80pCMiEhIKfBGRkFDgi4iERFKP4YuInE5bWxs1NTW0tLT4XYrnMjIyKC0tJTU1tdfLKPBFZNCqqakhNzeXMWPG0K23VuA556ivr6empoaxY8f2ejkN6YjIoNXS0kJhYWGowh7AzCgsLDzr/7JR4IvIoBa2sO/Sl/0OXOC3d3TyxNJtvP52nd+liIgklcAFfjRizHt9B4s27fO7FBEJgUWLFnHeeedRUVHBY4899qHft2zZwsyZM0lPT+e73/2uDxV+IHAnbc2M8SU5bNt/2O9SRCTgOjo6uPfee1m8eDGlpaVMnz6dOXPmMHHixOPzDBkyhB/96Ef85je/8a/QuMAd4QNUlOTwdu0h1AlURBJpxYoVVFRUUF5eTlpaGjfddBMLFiw4YZ6SkhKmT59+VpdPJkrgjvABKkpyaTyyi/rmVopyzvT0QxEJgof/axNVew4O6Donjsjjob+YdMrfd+/eTVlZ2fHvpaWlLF++fEBrGEiBPMIfX5IDwLZaDeuISOL0NIqQzFcNBfIIf/zQWOBX1x5mRnmhz9WIiBdOdySeKKWlpezatev495qaGkaMGOF5Hb0VyCP8YXkZ5KSnsG3/Ib9LEZEAmz59OtXV1ezcuZPW1lZ++ctfMmfOHL/LOqVAHuGbGeNKcqjWkI6IJFBKSgqPP/4411xzDR0dHdx5551MmjSJn/zkJwDcc8897Nu3j8rKSg4ePEgkEuEHP/gBVVVV5OXleV+v51v0yPiSHN18JSIJN3v2bGbPnn3CtHvuuef452HDhlFTU+N1WT0K5JAOxAK/9tAxmo60+V2KiEhS8DTwzewdM9tgZmvNLKHPLuw6cbutTuP4IiLgzxH+x51z05xzlYncSEVxLgDVuuNWRAQI8JDOyHMyyUiN6MStiEic14HvgFfNbLWZ3d3TDGZ2t5mtMrNVdXV9P+kajRjjinN085WISJzXgX+Zc+4jwHXAvWZ2xckzOOfmOecqnXOVxcXF/drY+BIFvohIF08D3zm3J/5eC7wAXJzI7VWU5LC78SiHj7UncjMiEmJnao/snOP++++noqKCKVOmsGbNGiD2tK6LL76YqVOnMmnSJB566KGE1+pZ4JtZtpnldn0GZgEbE7nNipLYidvtOsoXkQToao+8cOFCqqqqmD9/PlVVVSfMs3DhQqqrq6murmbevHl86UtfAiA9PZ3f//73rFu3jrVr17Jo0SKWLVuW0Hq9PMIfCvzRzNYBK4CXnHOLErnB45dmKvBFJAF60x55wYIFfOELX8DMmDFjBo2NjezduxczIycnllFtbW20tbUlvPGaZ3faOud2AFO92h7A6CFZpEZNV+qIhMHCB2HfhoFd57AL4LoPD9N06U175J7m2b17N8OHD6ejo4OLLrqIbdu2ce+993LJJZcMbP0nCexlmQAp0Qhji7LZVqubr0Rk4PWmPfLp5olGo6xdu5aamhpWrFjBxo0JHeUObi+dLuNLctm4p8nvMkQk0U5zJJ4ovWmP3Jt5CgoKuPLKK1m0aBGTJ09OWL2BPsKH2JU6uxqO0NLW4XcpIhIwvWmPPGfOHJ599lmccyxbtoz8/HyGDx9OXV0djY2NABw9epTXXnuNCRMmJLTe4B/hD82h08GOumYmjvC+HamIBFdv2iPPnj2bl19+mYqKCrKysnjmmWcA2Lt3L7fffjsdHR10dnZy4403cv311ye23oSuPQmMj1+aWV17SIEvIgPuTO2RzYy5c+d+aLkpU6bw1ltvJby+7gI/pDOmKIuI6dJMEZHAB356SpQxhdkKfBEJvcAHPsRO3OpafBEJu1AE/vihObxzoJnW9k6/SxER8U0oAr+iJIf2Tse79c1+lyIi4ptQBP4HV+poWEdEwisUgT+uOAfTlToikgB9bY8McOedd1JSUpLQu2u7C0XgZ6ZFKT0nU0f4IjKg+tMeGeCOO+5g0aKENg0+QSgCH6CiOIfq/WqiJiIDpz/tkQGuuOIKhgwZ4lm9gb/Ttsv4obn8aXs9HZ2OaCSxPadFxHv/suJf2NKwZUDXOWHIBP7+4r8/5e/9bY/stfAc4Zfk0Nreya6GI36XIiIB0d/2yF4LzxF+SezJMtW1hxlTlO1zNSIy0E53JJ4oA9Ue2SuhOcIfdzzwNY4vIgOjP+2R/RCawM/LSGVYXgbb9utKHREZGN3bI59//vnceOONx9sjd7VInj17NuXl5VRUVHDXXXfxxBNPHF/+5ptvZubMmWzdupXS0lKeeuqphNZrPY0vJYvKykq3atWqAVvfF55eQd2hYyz8yuUDtk4R8c/mzZs5//zz/S7DNz3tv5mtds5V9jR/aI7wAS4YmUf1/kN6+pWIhFLIAr+A9k7H5r0H/S5FRMRzoQr8KaX5AGzYrYeai0j4hCrwh+dnUJSTxvoaBb6IhE+oAt/MuGBkPhsU+CISQqEKfIALSguorj3EkdZ2v0sREfFU6AJ/ysh8Oh1U7dGJWxHpv/60Rz7Vst/+9reZMmUK06ZNY9asWezZs2dAag1d4F8QP3GrcXwR6a/+tEc+3bIPPPAA69evZ+3atVx//fU88sgjA1Jv6AJ/aF4GQ/PSdaWOiPRbf9ojn27ZvLy848s3NzcPWLM1z5unmVkUWAXsds5d7/X2IXY9/vqaRj82LSIJsu+f/oljmwe2PXL6+RMY9s1vnvL3/rRHPtOy3/rWt3j22WfJz89nyZIlA7E7vhzhfwXY7MN2j5tSms+OA80camnzswwRGeT60x75TMs++uij7Nq1i1tvvZXHH398AKr1+AjfzEqBTwGPAl/zctvdXVCaj3Owac9BZpQX+lWGiAyg0x2JJ0p/2iO3trb2qm3yLbfcwqc+9Skefvjhftfr9RH+D4BvAJ0eb/cEF4yM33GrE7ci0g/9aY98umWrq6uPL//iiy8yYcKEAanXsyN8M7seqHXOrTazK08z393A3QCjRo1KSC1FOemMLMhkvU7cikg/dG+P3NHRwZ133nm8PTLAPffcw+zZs3n55ZepqKggKyuLZ5555rTLAjz44INs3bqVSCTC6NGjj6+vvzxrj2xm/wzcBrQDGUAe8Gvn3OdPtcxAt0fu7p7/s5ot+w6y9IGPJ2T9IpJ4ao+cpO2RnXP/4Jwrdc6NAW4Cfn+6sE+0C0rzeaf+CE1HdOJWRMIhdNfhd+nqnLlxj4Z1RCQcfAl859xSv67B79J14lZ33IoMbsn81L5E6st+h/YIvyArjVFDstiwu9HvUkSkjzIyMqivrw9d6DvnqK+vJyMj46yW8/xO22RyQWk+63Y1+l2GiPRRaWkpNTU11NXV+V2K5zIyMigtLT2rZUId+FNG5vPS+r00NLcyJDvN73JE5CylpqYyduxYv8sYNEI7pAMfdM5UIzURCYNQB/7k43fcNvpbiIiIB0Id+HkZqZQXZetKHREJhVAHPsSGdTSkIyJhoMAfmc/ephZqD7X4XYqISEKFPvCnlBYAsFFH+SIScKEP/Ekj8jDTHbciEnyhD/zs9BQqinPUG19EAi/0gQ/xO25rmkJ3e7aIhIsCH7ho9DkcOHyMnQea/S5FRCRhFPhw/Lm2y3Y0+FyJiEjiKPCB8qJsSnLTeXNHvd+liIgkjAIfMDNmlBeybEf42qyKSHgo8ONmjiuk7tAxttdpHF9EgkmBHzczPo6vYR0RCSoFftzowiyG5WWwTIEvIgGlwI8zM2aOK2S5xvFFJKAU+N3MLC/kwOFWqmsP+12KiMiAU+B388H1+BrWEZHgUeB3UzYkk5EFmby5XYEvIsGjwO+m63r85Tsb6OzUOL6IBIsC/yQzyofQ0NzK27WH/C5FRGRAKfBPMnNc/Hp8DeuISMAo8E9Sek4WZUMydeJWRAJHgd+DGWM1ji8iwaPA78HMcYU0Hmljyz6N44tIcCjwezBDfXVEJIA8C3wzyzCzFWa2zsw2mdnDXm37bI0oyGR0YZZO3IpIoHh5hH8MuMo5NxWYBlxrZjM83P5ZmVleyIqd9XRoHF9EAsKzwHcxXU1qUuOvpE3TmeMKOdjSzua9B/0uRURkQHg6hm9mUTNbC9QCi51zy3uY524zW2Vmq+rq6rws7wTHx/E1rCMiAeFp4DvnOpxz04BS4GIzm9zDPPOcc5XOucri4mIvyzvB0LwMyouydT2+iATGWQe+mWWbWbQ/G3XONQJLgWv7s55EmzGukBU7G2jv6PS7FBGRfjtj4JtZxMxuMbOXzKwW2ALsjV9p829mNr43GzKzYjMriH/OBK6OrytpzSwv5NCxdtbVNPldiohIv/XmCH8JMA74B2CYc67MOVcCXA4sAx4zs8/3Yj3DgSVmth5YSWwM/7d9rNsTV5xbTErEWFy13+9SRET6LaUX81ztnGs7eaJzrgF4HnjezFLPtBLn3HrgwrMv0T/5manMHFfIq1X7ePC6CX6XIyLSL705wh9pZv9qZr82syfN7MtmNrr7DD39QQiKWROHsqOumW1qlywig1xvAn8BsBWYC3wSmAq8bmZzzSw9kcUlg09OHAbAK5s0rCMig1tvAj/qnHvKOfc7oME5dxexMf13gHmJLC4ZDMvPYGpZAa9qHF9EBrneBP5rZvbl+GcH4Jxrd879GzAzYZUlkVkTh7JuVyP7mlr8LkVEpM96E/hfA/LNbBUwIn4n7OfNbC4QiruSrpk0FIDFVft8rkREpO/OGPjOuU7n3KPAFcDdwDDgImAjcF1iy0sOFSW5lBdna1hHRAa1M16WaWYWb3x2BHgx/upxnkQUmCxmTRzGk2/soOloG/mZZ7wKVUQk6fTqxiszu8/MRnWfaGZpZnaVmf0cuD0x5SWPWZOG0t7pWLKl1u9SRET6pDeBfy3QAcw3sz1mVmVmO4Fq4GbgfznnfpbAGpPCtNICSnLTeVXj+CIySJ1xSMc51wI8ATwRv6O2CDgab4AWGpGI8cmJQ3nhrd20tHWQkdqv/nEiIp47q26Zzrk259zesIV9l1mThnGktYM/bTvgdykiImetN90yD5nZwVO86sxsmZl9woti/TazvJDc9BRe1V23IjII9WZIJ/dUv8X74k8Gnou/B1paSoQrJ5Tw2ub9dHQ6ohHzuyQRkV7r1xOv4k+wWgf8+wDVk/SumTSU+uZW1rz3vt+liIiclQF5xKFz7qcDsZ7B4GPnFpMWjfDKRl2tIyKDi6fPtA2C3IxULq0o5NWq/QT8XjMRCRgFfh/MmjiM9xqOsHW/euSLyOChwO+DT04cSsRgwdo9fpciItJrCvw+KM5N5+PnlfCr1TW0d3T6XY6ISK8o8Pvoc9PLqDt0jCVb6/wuRUSkVxT4ffTxCSUU56bzHyt3+V2KiEivKPD7KDUa4a8/UsqSrbXUHtSTsEQk+Snw++Fz08vo6HT8ak2N36WIiJyRAr8fxhZlc/HYIfznyl26Jl9Ekp4Cv59uml7GO/VHWL6zwe9SREROS4HfT9dNHk5uRopO3opI0lPg91NmWpQbpo3g5Q17aTra5nc5IiKnpMAfADdNH8Wx9k5eXLvb71JERE5JgT8AJo/MZ+LwPH6pYR0RSWIK/AFy08VlbNpzkI27m/wuRUSkR54FvpmVmdkSM9tsZpvM7CtebdsLN0wdSXpKRCdvRSRpeXmE3w583Tl3PjADuNfMJnq4/YTKz0rlusnD+M3a3bS0dfhdjojIh3gW+M65vc65NfHPh4DNwEivtu+Fz00fxaGWdhZu3Ot3KSIiH+LLGL6ZjQEuBJb38NvdZrbKzFbV1Q2uTpQzyodQXpTN0398R3feikjS8TzwzSwHeB74qnPu4Mm/O+fmOecqnXOVxcXFXpfXL2bG336snA27m/jD24Prj5WIBJ+ngW9mqcTC/jnn3K+93LZX/vLCUkbkZzB3yTa/SxEROYGXV+kY8BSw2Tn3fa+267W0lAh/+7FxrHznfZbvqPe7HBGR47w8wr8MuA24yszWxl+zPdy+Zz43vYyinHQe11G+iCSRFK825Jz7I2Bebc9PGalR7rp8LP+8cAvrdjUytazA75JERHSnbaLcOmM0+ZmpOsoXkaShwE+QnPQU/uayMSyu2s+WfR+6GElExHMK/AS649IxZKdFmbtku9+liIgo8BOpICuN22aO4aX1e9hRd9jvckQk5BT4CfbFj44lNRrhx0t1lC8i/lLgJ1hxbjo3XzyKF97aTc37R/wuR0RCTIHvgbuvKMcMfvqHHX6XIiIhpsD3wIiCTG6sLGP+iveo3n/I73JEJKQU+B752ifPJSstykMvblInTRHxhQLfI4U56TxwzXn8eXs9L21Qv3wR8Z4C30O3XDKaSSPyePSlzTQfa/e7HBEJGQW+h6IR45EbJrG3qUUtF0TEcwp8j100egh//ZFSnnxjB9t1M5aIeEiB74MHr5tARmqU/6kTuCLiIQW+D4pz0/naJ8/ljeoDvLJpv9/liEhIKPB9ctuM0UwYlst3flvF0dYOv8sRkRBQ4PskJRrh4TmT2N14lB8v1QlcEUk8Bb6PLikv5NPTRvCTP+xg054mv8sRkYBT4Pvsf/zFJM7JTuW+X7yla/NFJKEU+D4bkp3GD2+6kHfqm/n2go1+lyMiAabATwIzygu5/xPj+fWa3Ty/usbvckQkoBT4SeK+q8ZzydghfHvBRt2QJSIJocBPEtGI8cObLiQjNcq9z62hpU2XaorIwFLgJ5Fh+Rl877NT2bLvEP/08ma/yxGRgFHgJ5mPTyjhrsvH8uyb77Joo9ooi8jAUeAnoQeumcDUsgK+8av17DzQ7Hc5IhIQCvwklJYS4fGbLyQlGuG2p5ZTe7DF75JEJAAU+EmqbEgWz9wxnYbmVr7w9Aqajrb5XZKIDHIK/CQ2tayAn952EdvrDnPXs6t05Y6I9ItngW9mT5tZrZnpdtKzcPn4Yr534zRWvtPAffPfor2j0++SRGSQ8vII/2fAtR5uLzDmTB3BQ9dPZHHVfr71wkY9NEVE+iTFqw055143szFebS9o7rhsLAcOt/L4km0U5abxwDUT/C5JRAYZzwK/t8zsbuBugFGjRvlcTXL5+qxzqW8+xtwl20lPiXLfVRWYmd9licggkXSB75ybB8wDqKys1NhFN2bGP376Ao61dfL9xW+z72ALj8yZREpU595F5MySLvDl9KIR43s3TmVofgY/Xrqd2oPH+PebLyQzLep3aSKS5HRoOAiZGX9/7QQenjOJ323Zzy1PLqOhudXvskQkyXl5WeZ84E3gPDOrMbMverXtoLr90jH8+NaPsGnPQT7z4z+zq+GI3yWJSBLzLPCdczc754Y751Kdc6XOuae82naQXTt5OM/9t0uob27lL5/4M+trGv0uSUSSlIZ0AmD6mCE8/6WZpKdE+MyP3+TJN3bQ2anz3SJyIgV+QFSU5PLb+z7Kx84r5h9f2szf/GwldYeO+V2WiCQRBX6AnJOdxrzbLuI7n57Msh31XPfD11m6tdbvskQkSSjwA8bMuG3GaF788kcpzE7njmdW8sh/VXGsXY3XRMJOgR9Q5w3LZcGXL+P2maN5+k87mfPvf+LN7fV+lyUiPlLgB1hGapSHb5jM03dUcvhYOzf/72Xc+4s17G486ndpIuIDBX4IXDVhKK997WN89erxvFa1n098byk/+l21+uuLhIwCPyQy06J89epz+d3XP8YnJgzl+4vf5urv/4GFG/aq3bJISCjwQ6b0nCzm3voRfnHXJWSnpfCl59Zw3Q/f4MV1e+jQtfsigabAD6lLxxXx0v0f5fs3TqW903H//Lf4xPeWMn/Fe7qiRySgLJn/c76ystKtWrXK7zICr7PT8WrVfuYu2caG3U0MzUvnrsvL+WxlGfmZqX6XJyJnwcxWO+cqe/xNgS9dnHP8cdsB5i7ZxrIdDaSnRJh9wXBurCxjRvkQPWxFZBA4XeCrH74cZ2ZcPr6Yy8cXs6Gmif9Y9R4L3trDC2/tZnRhFjdWlvGZi0oZmpfhd6ki0gc6wpfTOtrawcKNe/mPlbtYvrOBiMGM8kKumTSMWZOGMjw/0+8SRaQbDenIgNh5oJlfrd7Fwo372FHXDMDU0nxmTRrGNZOGUlGS63OFIqLAlwG3rfYwr2zax6ub9rGupgmAUUOyuHRcITPjr5JcDf2IeE2BLwm1t+koi6v280b1AZbtqOdQSzsA40tyuHRcIZeUFzKtrIDh+Rk68SuSYAp88UxHp2PTnib+vL2eP2+vZ+XOBo7GWzgU5aQzrSyfqaUFTC0rYEppPgVZaT5XLBIsCnzxTWt7J1V7D7K+ppG1uxpZt6uR7fHxf4BheRmcOyyX84bmcO7QXM4blktFSQ5ZabqATKQvFPiSVA62tLGxpon1u5t4e/8h3t5/iOr9hznW3gmAGQzPy2B0YTZjirJi74Wx99JzMsnN0M1gIqei6/AlqeRlpHJpRRGXVhQdn9bR6Xiv4Qhb98X+ALxT38y79UdYXLWfA4dbT1g+Nz2F4QUZjCjIZHh+JiMLMhial0FxbjolubH3IdlpRCM6XyDSnQJfkkI0YowtymZsUTbXTh52wm+HWtp4t/4I79YfYXfjEfY0trCn8Sh7mo6yoaaJ+ubWHtdXmJ1GUU4s/M/JTmNIVmrsPTuNgqw08jNTyctIIT8zNfY5M5XUqNpLSXAp8CXp5WakMnlkPpNH5vf4e0tbB7UHj1F3uCX+foy6Qx+83j/Syu7GozQ0t9J0tO2028pKi5KTnkJORkrsPT2F7PQUctNTyEqPkpWWQmZqlOz0KJlpKWSlRslKi5KR2vWKkJEaJTP+PT0lQlpKhPSUCCn6YyI+U+DLoJeRGmVUYRajCrPOOG97RyeNR9t4v7mVgy1tNB1t4+DR9vh77PvhY+3HX83H2tnVcITDx9o52tpBc2s7LW2dfaozGjHSohHSUyOkRSOkRmN/DNKiEVJTYr+lRCPxdyM1GiE1/p4SiZASsePTo12fIxEiESMlYkTjr+6fIxb73n2eiMVe0Qjx9/i0iBExiJphFvscOT4/x5ezrs/x5Q0+mL/b7xBb3vhgete83T9HDIzYhBPmoet7bH5O+n7yfMS/y6kFMvCfvnUiJbXJezJakksKUBh/JVQf/pFM1D/FnfGXJKe64gh3zt804OsNZOAXpOSQFTnmdxkJpD9mchLX238qXA+fTjlLL9bincRcUJic/y5lWXpC1hvIwP+rn6/wuwQRkaSjs0giIiGhwBcRCQkFvohISHga+GZ2rZltNbNtZvagl9sWEQk7zwLfzKLAXOA6YCJws5lN9Gr7IiJh5+UR/sXANufcDudcK/BL4AYPty8iEmpeBv5IYFe37zXxaScws7vNbJWZraqrq/OsOBGRoPMy8Hu65/lDdz045+Y55yqdc5XFxcUelCUiEg5e3nhVA5R1+14K7DndAqtXrz5gZu/2cXtFwIE+LjuYab/DRfsdLr3Z79Gn+sGzB6CYWQrwNvAJYDewErjFOTfwDSNi21t1qocABJn2O1y03+HS3/327AjfOdduZl8GXgGiwNOJCnsREfkwT3vpOOdeBl72cpsiIhIT5Dtt5/ldgE+03+Gi/Q6Xfu13Uj/EXEREBk6Qj/BFRKQbBb6ISEgELvDD1KDNzJ42s1oz29ht2hAzW2xm1fH3c/yscaCZWZmZLTGzzWa2ycy+Ep8e9P3OMLMVZrYuvt8Px6cHer+7mFnUzN4ys9/Gv4dlv98xsw1mttbMVsWn9XnfAxX4IWzQ9jPg2pOmPQj8zjk3Hvhd/HuQtANfd86dD8wA7o3/fxz0/T4GXOWcmwpMA641sxkEf7+7fAXY3O17WPYb4OPOuWndrr/v874HKvAJWYM259zrQMNJk28Afh7//HPg017WlGjOub3OuTXxz4eIhcBIgr/fzjl3OP41Nf5yBHy/AcysFPgU8GS3yYHf79Po874HLfB71aAt4IY65/ZCLByBEp/rSRgzGwNcCCwnBPsdH9ZYC9QCi51zodhv4AfAN4DObtPCsN8Q+6P+qpmtNrO749P6vO9Be4h5rxq0yeBnZjnA88BXnXMHzXr6vz5YnHMdwDQzKwBeMLPJPpeUcGZ2PVDrnFttZlf6XI4fLnPO7TGzEmCxmW3pz8qCdoR/1g3aAmi/mQ0HiL/X+lzPgDOzVGJh/5xz7tfxyYHf7y7OuUZgKbHzN0Hf78uAOWb2DrEh2qvM7P8S/P0GwDm3J/5eC7xAbNi6z/setMBfCYw3s7FmlgbcBLzoc01eexG4Pf75dmCBj7UMOIsdyj8FbHbOfb/bT0Hf7+L4kT1mlglcDWwh4PvtnPsH51ypc24MsX+ff++c+zwB328AM8s2s9yuz8AsYCP92PfA3WlrZrOJjfl1NWh71N+KEsfM5gNXEmuZuh94CPgN8J/AKOA94LPOuZNP7A5aZvZR4A1gAx+M6X6T2Dh+kPd7CrETdFFiB2r/6Zx7xMwKCfB+dxcf0vnvzrnrw7DfZlZO7KgeYsPvv3DOPdqffQ9c4IuISM+CNqQjIiKnoMAXEQkJBb6ISEgo8EVEQkKBLyISEgp8CSwzOxx/H2Nmtwzwur950vc/D+T6RRJBgS9hMAY4q8CPd149nRMC3zl36VnWJOI5Bb6EwWPA5fGe4n8Xb0L2b2a20szWm9nfQuzGnniv/V8Qu7ELM/tNvHHVpq7mVWb2GJAZX99z8Wld/zVh8XVvjPcx/1y3dS81s1+Z2RYzey5+1zBm9piZVcVr+a7n/+tIaASteZpITx4kfocmQDy4m5xz080sHfiTmb0an/diYLJzbmf8+53OuYZ4O4OVZva8c+5BM/uyc25aD9v6K2L96qcSuwN6pZm9Hv/tQmASsf5OfwIuM7Mq4C+BCc4519U+QSQRdIQvYTQL+EK81fByoBAYH/9tRbewB7jfzNYBy4g15hvP6X0UmO+c63DO7Qf+AEzvtu4a51wnsJbYUNNBoAV40sz+CjjSz30TOSUFvoSRAffFnyI0zTk31jnXdYTffHymWO+Wq4GZ8SdNvQVk9GLdp3Ks2+cOIMU5107svyqeJ/Ygi0VnsR8iZ0WBL2FwCMjt9v0V4EvxNsuY2bnxboQnywfed84dMbMJxB6p2KWta/mTvA58Ln6eoBi4AlhxqsLiff3znXMvA18lNhwkkhAaw5cwWA+0x4dmfgb8kNhwypr4idM6en5M3CLgHjNbD2wlNqzTZR6w3szWOOdu7Tb9BWAmsI7Yw3e+4ZzbF/+D0ZNcYIGZZRD7r4O/69MeivSCumWKiISEhnREREJCgS8iEhIKfBGRkFDgi4iEhAJfRCQkFPgiIiGhwBcRCYn/D4vmbvx3ZZgxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotConvergenceGraph(X, y, iterations=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing the normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closed-form solution to linear regression is\n",
    "$$ \\theta = \\left( X^T X\\right)^{-1} X^T\\vec{y}$$\n",
    "\n",
    "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no “loop until convergence” like in gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEqn(X, y):\n",
    "    \"\"\"\n",
    "    Computes the closed-form solution to linear regression using the normal equations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        The value at each data point. A vector of shape (m, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        Estimated linear regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the code to compute the closed form solution to linear\n",
    "    regression and put the result in theta.\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    Look up the function `np.linalg.pinv` for computing matrix inverse.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normal equation\n",
    "    term_1 = np.linalg.inv(X.T.dot(X))\n",
    "    term_2 = (X.T.dot(y))\n",
    "    \n",
    "    return term_1.dot(term_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('ex1data2.txt', header=None, names=['Area', 'Bedrooms', 'Price'])\n",
    "\n",
    "# Format features\n",
    "m = len(df)\n",
    "X = np.column_stack((np.ones(m),\n",
    "                     df['Area'].to_numpy(),\n",
    "                     df['Bedrooms'].to_numpy()\n",
    "                    ))\n",
    "y = df['Price'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta found by gradient descent: 89597.91, 139.21, -8738.02\n"
     ]
    }
   ],
   "source": [
    "theta = normalEqn(X, y)\n",
    "print(f'Theta found by gradient descent: {theta[0]:.2f}, {theta[1]:.2f}, {theta[2]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): $293081.46\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array((1, 1650, 3))\n",
    "price = hypothesis(X_test, theta)\n",
    "\n",
    "print (f'Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ${price:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Linear regression\n",
    "\n",
    "The code below represents my implementation of linear regression with Python. **Vectorized** and **unvectorized** solutions are provided, with the latter used to further solidify my intitution for this exercise.\n",
    "\n",
    "Jupyter notebook template modified from: https://github.com/dibgerge/ml-coursera-python-assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple python and numpy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmUpExercise(x):\n",
    "    \"\"\"\n",
    "    Example function in Python which computes the identity matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : int\n",
    "        The input will specify size of identity matrix. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : array_like\n",
    "        The 5x5 identity matrix.\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    return np.identity(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (warmUpExercise(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reading and plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('ex1data1.txt', header=None, names =['x', 'y'])\n",
    "x = df.iloc[:,0]\n",
    "y = df.iloc[:,1]\n",
    "m = len(x)\n",
    "\n",
    "# Create numpy arrays\n",
    "X = np.column_stack((np.ones(len(x)), x.to_numpy()))\n",
    "y = y.to_numpy()\n",
    "\n",
    "#Plot data\n",
    "fig = plt.scatter(x, y)\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.xlabel('Population of City in 10,000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize the following cost function: <br>\n",
    "<br>\n",
    "<br>\n",
    "<center> $\\Large J(\\theta) = \\frac{1}{2m} \\sum^{m}_{i=1}(h_\\theta(x^{(i)}) - y^{(i)})^2 $ </center>\n",
    "<br>\n",
    "<br>\n",
    "where the hypothesis $h_\\theta(x)$ is defined by the linear model:\n",
    "<br>\n",
    "<br>\n",
    "<center> $\\Large h_\\theta(x) = \\theta^TX = \\theta_0x_0 + \\theta_1x_1 $ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Hypothesis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X, theta):\n",
    "    \"\"\"\n",
    "    Computes the hypothesis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features. \n",
    "    \n",
    "    theta : np.array\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (n, 1).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    hypothesis : np.array\n",
    "        Prediction using theta and X.\n",
    "    \n",
    "    \"\"\" \n",
    "    return X.dot(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute cost for linear regression. \n",
    "    Computes the cost of using theta as the parameter for \n",
    "    linear regression to fit the data points in X and y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features.\n",
    "    \n",
    "    y : np.array\n",
    "        The values of the function at each data point. \n",
    "        This is a vector of shape (m, ).\n",
    "    \n",
    "    theta : np.array\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (n, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The value of the regression cost function.\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    m = len(X)\n",
    "    h = hypothesis(X, theta)\n",
    "    \n",
    "    J = np.sum(np.square(h - y)) / (2 * m)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the cost function\n",
    "J = computeCost(X, y, theta=np.zeros(2))\n",
    "print(f'With theta = [0, 0] \\nCost computed = {J:.2f}')\n",
    "print('Expected cost value (approximately) 32.07\\n')\n",
    "\n",
    "# further testing of the cost function\n",
    "J = computeCost(X, y, theta=np.array([-1, 2]))\n",
    "print(f'With theta = [-1, 2]\\nCost computed = {J:.2f}')\n",
    "print('Expected cost value (approximately) 54.24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5 Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `theta`. Updates theta by taking \n",
    "    `iterations` gradient steps with learning rate `alpha`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features.\n",
    "    \n",
    "    y : np.array\n",
    "        The values of the function at each data point. \n",
    "        This is a vector of shape (m, ).\n",
    "    \n",
    "    theta : np.array\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (n, ).\n",
    "    \n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "    \n",
    "    iterations : int\n",
    "        The number of iterations for gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : np.array\n",
    "        The learned linear regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    J_history : np.array\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(X)\n",
    "    J_hist = np.zeros(iterations)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        h = hypothesis(X, theta)\n",
    "        theta = theta - ((h - y).dot(X) * (alpha / m))\n",
    "        J_hist[i] = computeCost(X, y, theta)\n",
    "        \n",
    "    return theta, J_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fitting parameters\n",
    "theta = np.zeros(2)\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "theta, J_history = gradientDescent(X ,y, theta, alpha, iterations)\n",
    "print(f'Theta found by gradient descent: {theta[0]:.4f}, {theta[1]:.4f}')\n",
    "print(f'Expected theta values (approximately): [-3.6303, 1.1664]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.6 Plot linear regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear(X, y, theta):\n",
    "    \"\"\"\n",
    "    Plots linear regression line to data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features.\n",
    "    \n",
    "    y : np.array\n",
    "        The values of the function at each data point. \n",
    "        This is a vector of shape (m, ).\n",
    "    \n",
    "    theta : np.array\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (n, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fig : matplotlib object\n",
    "        A scatter plot of values and linear fiture.\n",
    "        \n",
    "    \"\"\"\n",
    "    plt.scatter(X[:,1], y)\n",
    "    plt.plot(X[:,1], theta[0] + theta[1]*X[:,1], 'c')\n",
    "    plt.ylabel('Profit in $10,000s')\n",
    "    plt.xlabel('Population of City in 10,000s')\n",
    "    plt.legend(['Training data', 'Linear regression'])\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_linear(X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values for population sizes of 35,000 and 70,000\n",
    "predict_1 = np.dot([1, 3.5], theta)\n",
    "print(f'For population = 35,000, we predict a profit of {predict_1*10000:.2f}\\n')\n",
    "\n",
    "predict_2 = np.dot([1, 7], theta)\n",
    "print(f'For population = 70,000, we predict a profit of {predict_2*10000:.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualizing $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of these graphs is to show how $J(\\theta)$ varies with changes in $\\theta_0$ and $\\theta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid over which we will calculate J\n",
    "theta0_vals = np.linspace(-10, 10, 100)\n",
    "theta1_vals = np.linspace(-1, 4, 100)\n",
    "\n",
    "# initialize J_vals to a matrix of 0's\n",
    "J_vals = np.zeros((theta0_vals.shape[0], theta1_vals.shape[0]))\n",
    "\n",
    "# Fill out J_vals\n",
    "for i, theta0 in enumerate(theta0_vals):\n",
    "    for j, theta1 in enumerate(theta1_vals):\n",
    "        J_vals[i, j] = computeCost(X, y, [theta0, theta1])\n",
    "        \n",
    "# Because of the way meshgrids work in the surf command, we need to\n",
    "# transpose J_vals before calling surf, or else the axes will be flipped\n",
    "J_vals = J_vals.T\n",
    "\n",
    "# surface plot\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='RdGy', alpha=0.7, cstride=2)\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'$\\theta_1$')\n",
    "plt.title('Surface')\n",
    "\n",
    "# contour plot\n",
    "# Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n",
    "ax = plt.subplot(122)\n",
    "plt.contour(theta0_vals, theta1_vals, J_vals, linewidths=1, cmap='RdGy', levels=np.logspace(-2, 3, 20))\n",
    "plt.contourf(theta0_vals, theta1_vals, J_vals, cmap='RdGy', alpha=0.3, levels=np.logspace(-2, 3, 20))\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'$\\theta_1$')\n",
    "plt.plot(theta[0], theta[1], 'ro', ms=5, lw=2)\n",
    "plt.title('Contour, showing minimum')\n",
    "plt.colorbar()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Implementing gradient descent - Unvectorized solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Hypothesis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesisLoop(X, theta):\n",
    "    \"\"\"\n",
    "    Computes the hypothesis using loops.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features. \n",
    "    \n",
    "    theta : np.array\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (n, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    hypothesis : np.array\n",
    "        Prediction using theta and X.\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    m = len(X)\n",
    "    n = X.shape[1]\n",
    "    hypothesis = np.zeros(m)\n",
    "\n",
    "    for i in range(m):\n",
    "        term = np.zeros(n)\n",
    "        for j in range(n):\n",
    "            term[j] = X[i,j] * theta[j]\n",
    "        hypothesis[i] = np.sum(term)\n",
    "        \n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostLoop(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute cost for linear regression using loops. \n",
    "    Computes the cost of using theta as the parameter for \n",
    "    linear regression to fit the data points in X and y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features.\n",
    "    \n",
    "    y : np.array\n",
    "        The values of the function at each data point. \n",
    "        This is a vector of shape (m, ).\n",
    "    \n",
    "    theta : np.array\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (n, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The value of the regression cost function.\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    m = len(X)\n",
    "    h = hypothesisLoop(X, theta)\n",
    "    \n",
    "    difference = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        difference[i] = h[i] - y[i]\n",
    "    \n",
    "    J = np.sum(np.square(difference)) / (2 * m)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentLoop(X, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `theta` using loops. \n",
    "    Updates theta by taking `iterations` gradient steps with learning \n",
    "    rate `alpha`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features.\n",
    "    \n",
    "    y : np.array\n",
    "        The values of the function at each data point. \n",
    "        This is a vector of shape (m, ).\n",
    "    \n",
    "    theta : np.array\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (n, ).\n",
    "    \n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "    \n",
    "    iterations : int\n",
    "        The number of iterations for gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : np.array\n",
    "        The learned linear regression parameters. A vector of shape (n+1, 1).\n",
    "    \n",
    "    J_history : np.array\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(X)\n",
    "    n = X.shape[1]\n",
    "    J_hist = np.zeros(iterations)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        h = hypothesisLoop(X, theta)\n",
    "        \n",
    "        cumulative_term = np.zeros(n)\n",
    "        for j in range(n):\n",
    "            for i in range(m):\n",
    "                cumulative_term[j] += (h[i] - y[i]) * X[i,j]\n",
    "                \n",
    "        for i in range(n):\n",
    "            theta[i] = theta[i] - (cumulative_term[i] * (alpha / m))\n",
    "        \n",
    "        J_hist[i] = computeCostLoop(X, y, theta)\n",
    "    \n",
    "    return theta, J_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.5 Gradient descent  with more explicit loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentLoopExplict(X, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `theta` using loops. \n",
    "    Updates theta by taking `iterations` gradient steps with learning \n",
    "    rate `alpha`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The input dataset of shape (m x n+1), where m is the \n",
    "        number of examples, and n is the number of features.\n",
    "    \n",
    "    y : np.array\n",
    "        The values of the function at each data point. \n",
    "        This is a vector of shape (m, ).\n",
    "    \n",
    "    theta : np.array\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (n, ).\n",
    "    \n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "    \n",
    "    iterations : int\n",
    "        The number of iterations for gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : np.array\n",
    "        The learned linear regression parameters. A vector of shape (n+1, 1).\n",
    "    \n",
    "    J_history : np.array\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(X)\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    for _ in range(1):\n",
    "        h = hypothesisLoop(X, theta)\n",
    "        term = np.zeros((m, n))\n",
    "        \n",
    "        # difference = (h_theta - y)\n",
    "        difference = np.zeros(m)\n",
    "        for row, i in enumerate(h):\n",
    "            difference[row] = i - y[row]\n",
    "\n",
    "        # second_term = (h_theta - y)(x_j)\n",
    "        second_term = np.zeros((m,2))\n",
    "        for row, i in enumerate(X):\n",
    "            for column, j in enumerate(i): \n",
    "                second_term[row, column] = difference[row] * j # Update each feature\n",
    "\n",
    "        # theta_old = sum((h_theta - y)(x_j))\n",
    "        theta_new = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            theta_new[i] = np.sum(second_term[:,i]) # Sum for each feature\n",
    "\n",
    "        # update theta\n",
    "        for i in range(n):\n",
    "            theta[i] = theta[i] - theta_new[i] * (alpha / m) # Update for each theta\n",
    "        print (theta_new)\n",
    "\n",
    "theta = np.zeros(2)\n",
    "gradientDescentLoop(X, y, theta, alpha, iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
